{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import warnings\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from statistics import median, mean\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier,RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.metrics import RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir = \"data/\"\n",
    "\n",
    "employee_survey = pd.read_csv(dir + \"employee_survey_data.csv\")\n",
    "general = pd.read_csv(dir + \"general_data.csv\")\n",
    "manager_survey = pd.read_csv(dir + \"manager_survey_data.csv\")\n",
    "in_time = pd.read_csv(dir + \"in_time.csv\")\n",
    "out_time = pd.read_csv(dir + \"out_time.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4444)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#es\">Employe survey</a></li>\n",
    "    <li><a href=\"#ge\">General</a></li>\n",
    "    <li><a href=\"#ms\">Manager survey</a></li>\n",
    "    <li><a href=\"#it\">In Time</a></li>\n",
    "    <li><a href=\"#ot\">Out Time</a></li>\n",
    "    <li><a href=\"#ld\">Lien entre les dataset</a></li>\n",
    "    <li><a href=\"#cc\">Conclusion</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition des variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbEmploye = employee_survey[\"EmployeeID\"].max()\n",
    "print(\"Il y a\", nbEmploye, \"employés dans le dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employee survey\n",
    "<div id=\"es\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données contenues dans le set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_survey.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anaylse des données vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_survey.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Répartition des notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_survey_anonyme = employee_survey.drop([\"EmployeeID\"], axis=1, inplace=False)\n",
    "sns.boxplot(data=employee_survey_anonyme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=employee_survey_anonyme, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tendance des mauvais résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On regarde tout d'abord le nombre de gens qui sont insatisfait le l'environnement de travail et on remarque qu'ils représentent 38,5% des employés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BadEnvironmentSatisfaction = employee_survey[employee_survey['EnvironmentSatisfaction']<2.1].count()\n",
    "print(BadEnvironmentSatisfaction['EnvironmentSatisfaction'] / nbEmploye * 100)\n",
    "BadEnvironmentSatisfaction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait la même chose pour la satisfaction sur leurs travail et on obtient des résultats similaires, ce qui confirme les boîtes a moustache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BadJobSatisfaction = employee_survey[employee_survey['JobSatisfaction']<2.1].count()\n",
    "print(BadJobSatisfaction['JobSatisfaction'] / nbEmploye * 100)\n",
    "BadJobSatisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin on regarde l'équilibre entre la vie privée et professionelle, dans ce cas on obtient 28.5% des employés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BadWorkLifeBalance = employee_survey[employee_survey['WorkLifeBalance']<2.1].count()\n",
    "print(BadWorkLifeBalance['WorkLifeBalance'] / nbEmploye * 100)\n",
    "BadWorkLifeBalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on essaie de regarder les employés qui ont a la foie une balance mauvaise, et une satisfaction de leur travail et de leur environnement de travail faible on obtient 178 personne avec une très mauvaise expérience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BadEnvironmentSatisfaction = employee_survey[employee_survey['EnvironmentSatisfaction']<2.1]\n",
    "BadSatisfaction = BadEnvironmentSatisfaction[BadEnvironmentSatisfaction['JobSatisfaction']<2.1]\n",
    "BadAll = BadSatisfaction[BadSatisfaction['WorkLifeBalance']<2.1]\n",
    "BadAll[\"EmployeeID\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General\n",
    "<div id=\"ge\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données contenue dans le set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des données vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications nécéssaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables discrètes hierarchiques\n",
    "\n",
    "Ensuite, on remplace les valeurs dans \"BusinessTravel\" avec l'\"OrdinalEncoder\" car il y a un ordre hiérarchique entre les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder(categories=[['Non-Travel','Travel_Rarely','Travel_Frequently']])\n",
    "encoded = ord_enc.fit_transform(general[[\"BusinessTravel\"]])\n",
    "general.drop('BusinessTravel', axis=1)\n",
    "general = general.assign(BusinessTravel = encoded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder(categories=[['No','Yes']])\n",
    "encoded = ord_enc.fit_transform(general[[\"Attrition\"]])\n",
    "general.drop('Attrition', axis=1)\n",
    "general = general.assign(Attrition = encoded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder(categories=[['N','Y']])\n",
    "encoded = ord_enc.fit_transform(general[[\"Over18\"]])\n",
    "general.drop('Over18', axis=1)\n",
    "general = general.assign(Over18 = encoded) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables discrètes indépendantes\n",
    "Ensuite on remplace les valeurs dans \"Department\" avec le \"OneHotEncoder\" car il n'y a pas de hiérarchie entre les valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_enc = OneHotEncoder()\n",
    "encoded = one_enc.fit_transform(general[['Department']])\n",
    "dpt_enc = pd.DataFrame(encoded.toarray())\n",
    "dpt_enc = dpt_enc.rename(columns={0:'Department_HR', 1:'Department_Research & Development', 2:'Department_Sales'})\n",
    "general.drop('Department', axis=1, inplace=True)\n",
    "general = pd.concat([general, dpt_enc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_enc = OneHotEncoder()\n",
    "encoded = one_enc.fit_transform(general[['EducationField']])\n",
    "edu_enc = pd.DataFrame(encoded.toarray())\n",
    "edu_enc = edu_enc.rename(columns={\n",
    "    0:'Education_Human Resources', \n",
    "    1:'Education_Life Sciences', \n",
    "    2:'Education_Marketing', \n",
    "    3:'Education_Medical', \n",
    "    4:'Education_Other', \n",
    "    5:'Education_Technical Degreee'})\n",
    "general.drop('EducationField', axis=1, inplace=True)\n",
    "general = pd.concat([general, edu_enc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_enc = OneHotEncoder()\n",
    "encoded = one_enc.fit_transform(general[['Gender']])\n",
    "gen_enc = pd.DataFrame(encoded.toarray())\n",
    "gen_enc = gen_enc.rename(columns={0:'Female', 1:'Male'})\n",
    "general.drop('Gender', axis=1, inplace=True)\n",
    "general = pd.concat([general, gen_enc], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_enc = OneHotEncoder()\n",
    "encoded = one_enc.fit_transform(general[['JobRole']])\n",
    "job_enc = pd.DataFrame(encoded.toarray())\n",
    "job_enc = job_enc.rename(columns={\n",
    "    0:'Job_Healthcare Representative', \n",
    "    1:'Job_Human Resources', \n",
    "    2:'Job_Laboratory Technician', \n",
    "    3:'Job_Manager', \n",
    "    4:'Job_Manufacturing Director',\n",
    "    5:'Job_Research Director',\n",
    "    6:'Job_Research Scientist',\n",
    "    7:'Job_Sales Executive',\n",
    "    8:'Job_Sales Representative'})\n",
    "general.drop('JobRole', axis=1, inplace=True)\n",
    "general = pd.concat([general, job_enc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_enc = OneHotEncoder()\n",
    "encoded = one_enc.fit_transform(general[['MaritalStatus']])\n",
    "mar_enc = pd.DataFrame(encoded.toarray())\n",
    "mar_enc = mar_enc.rename(columns={0:'Divorced', 1:'Married', 2:'Single'})\n",
    "general.drop('MaritalStatus', axis=1, inplace=True)\n",
    "general = pd.concat([general, mar_enc], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe globale encoder fait strictement la même chose que les cellules précédentes a ceci près qu'elle peu être intégré au pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class global_encoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, data):\n",
    "        bus_enc = self.ordinal_encode(data['BusinessTravel'])\n",
    "        data.drop([\"BusinessTravel\"], axis=1, inplace=True)\n",
    "        att_enc = self.ordinal_encode(data['Attrition'])\n",
    "        data.drop([\"Attrition\"], axis=1, inplace=True)\n",
    "        ove_enc = self.ordinal_encode(data['Over18'])\n",
    "        data.drop([\"Over18\"], axis=1, inplace=True)\n",
    "\n",
    "        mar_enc = self.one_hot_encode(data['MaritalStatus'])\n",
    "        data.drop([\"MaritalStatus\"], axis=1, inplace=True)\n",
    "        dep_enc = self.one_hot_encode(data['Department'])\n",
    "        data.drop([\"Department\"], axis=1, inplace=True)\n",
    "        edu_enc = self.one_hot_encode(data['EducationField'])\n",
    "        data.drop([\"EducationField\"], axis=1, inplace=True)\n",
    "        job_enc = self.one_hot_encode(data['JobRole'])\n",
    "        data.drop([\"JobRole\"], axis=1, inplace=True)\n",
    "        gen_enc = self.one_hot_encode(data['Gender'])\n",
    "        data.drop([\"Gender\"], axis=1, inplace=True)\n",
    "\n",
    "        return pd.concat([data, mar_enc, bus_enc, att_enc, ove_enc, dep_enc, edu_enc, job_enc, gen_enc], axis=1)\n",
    "\n",
    "\n",
    "    def one_hot_encode(self, column):\n",
    "        one_enc = OneHotEncoder()\n",
    "        encoded = one_enc.fit_transform(column)\n",
    "        df_enc = pd.DataFrame(encoded.toarray())\n",
    "        return df_enc.rename(columns=column.unique())\n",
    "    \n",
    "    def ordinal_encode(self, column):\n",
    "        ord_enc = OrdinalEncoder(categories=column.unique())\n",
    "        encoded = ord_enc.fit_transform(column)\n",
    "        return pd.DataFrame(encoded.toarray())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse générales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombres de personnes qui ont quitté l'entreprise en 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleLeft = general[general[\"Attrition\"] == 1]\n",
    "print(\"Number of people who left the company:\", peopleLeft.shape[0], \"out of\", general.shape[0], \"employees\")\n",
    "print(\"Approx\", round(peopleLeft.shape[0] / general.shape[0] * 100, 2), \"% of the employees left the company\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grâce a cette HeatMap on remarque qu'il n'y a pas de réel corrélation significative entre les gens qui s'en vont et leurs situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 10))\n",
    "general_corr = general.corr()\n",
    "sns.heatmap(general_corr[['Attrition']].sort_values(by=['Attrition'],ascending=False), vmin=-1, vmax=1, annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.filter(items= [\"Age\",\"DistanceFromHome\",\"Education\",\"JobLevel\", \"MonthlyIncome\", \"NumCompaniesWorked\", \"PercentSalaryHike\", \"TotalWorkingYears\", \"TrainingTimesLastYear\", \"YearsAtCompany\", \"YearsSinceLastPromotion\", \"YearsWithCurrManager\"]).hist(figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manager survey\n",
    "<div id=\"ms\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données contenue dans le set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_survey.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des données vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_survey.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse générale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_survey_anonyme = manager_survey.drop([\"EmployeeID\"], axis=1, inplace=False)\n",
    "sns.boxplot(data=manager_survey_anonyme)\n",
    "\n",
    "#corréler avec attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Time\n",
    "<div id=\"it\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données contenues dans le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des données vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Par date\")\n",
    "print(in_time.isnull().sum())\n",
    "print(\"Par employé\")\n",
    "print(in_time.isnull().sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Répartition des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.boxplot(in_time.isnull().sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out Time\n",
    "<div id=\"ot\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données contenues dans le set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des données vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Par date\")\n",
    "print(out_time.isnull().sum())\n",
    "print(\"Par employé\")\n",
    "print(out_time.isnull().sum(axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Répartition des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.boxplot(in_time.isnull().sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lien entre des datasets\n",
    "<div id=\"ld\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données générales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base du temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayOff = in_time.isnull().sum() == 4410\n",
    "print(\"Il y a\", dayOff.sum() ,\"jours de congés\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average of the number of hours worked per day\n",
    "in_time = in_time.dropna(axis=1, how='all')\n",
    "out_time = out_time.dropna(axis=1, how='all')\n",
    "in_time = in_time.dropna(axis=0, how='all')\n",
    "out_time = out_time.dropna(axis=0, how='all')\n",
    "in_time = in_time.apply(pd.to_datetime)\n",
    "out_time = out_time.apply(pd.to_datetime)\n",
    "hours_worked = out_time - in_time\n",
    "hours_worked = hours_worked.applymap(lambda x: x.total_seconds() / 3600)\n",
    "hours_worked = hours_worked.mean(axis=1)\n",
    "hours_worked = hours_worked.dropna()\n",
    "hours_worked = hours_worked.reset_index(drop=True)\n",
    "sns.boxplot(hours_worked)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_worked_df = pd.DataFrame({\n",
    "    \"EmployeeID\": general[\"EmployeeID\"],\n",
    "    \"avg_hours_worked\": hours_worked })\n",
    "hours_worked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque un taux plus élevé de démissions chez les gens qui travaillent le plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_time = general.merge(hours_worked_df, on=\"EmployeeID\")\n",
    "\n",
    "sns.histplot(data=general_time, x=\"avg_hours_worked\", hue=\"Attrition\", multiple=\"stack\", bins=20, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_badAll = general_time.merge(BadAll, on=\"EmployeeID\", copy=True)\n",
    "\n",
    "(general_badAll[\"Attrition\"] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_timeAverage = out_time[out_time.columns[1:]].apply(pd.to_datetime)\n",
    "out_timeAverage = out_timeAverage.apply(lambda x: (x - x.dt.normalize()).dt.total_seconds() / 3600)\n",
    "out_timeAverage  = out_timeAverage.mean(axis=1)\n",
    "out_timeAverage = out_timeAverage.to_frame()\n",
    "out_timeAverage = out_timeAverage.assign(EmployeeID=range(1, len(out_timeAverage)+1))\n",
    "out_timeAverage = out_timeAverage.rename(columns={0: 'avg_out_time'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_timeAverage = in_time[in_time.columns[1:]].apply(pd.to_datetime)\n",
    "in_timeAverage = in_timeAverage.apply(lambda x: (x - x.dt.normalize()).dt.total_seconds() / 3600)\n",
    "in_timeAverage = in_timeAverage.mean(axis=1)\n",
    "in_timeAverage = in_timeAverage.to_frame()\n",
    "in_timeAverage = in_timeAverage.assign(EmployeeID=range(1, len(in_timeAverage)+1))\n",
    "in_timeAverage = in_timeAverage.rename(columns={0:'AverageStartTime'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = general.merge(manager_survey, on=\"EmployeeID\", copy=True)\n",
    "all_data = all_data.merge(employee_survey, on=\"EmployeeID\", copy=True)\n",
    "all_data = all_data.merge(hours_worked_df, on=\"EmployeeID\", copy=True)\n",
    "all_data = all_data.merge(out_timeAverage, on=\"EmployeeID\", copy=True)\n",
    "all_data = all_data.merge(in_timeAverage, on=\"EmployeeID\", copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data[\"EmployeeID\"].unique())\n",
    "print(all_data[\"Over18\"].unique())\n",
    "print(all_data[\"StandardHours\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataCleaning(all_data, type):\n",
    "    if type == 1:\n",
    "        # drop useless columns\n",
    "        all_data.drop(\"EmployeeID\", axis=1, inplace=True)\n",
    "        all_data.drop(\"Over18\", axis=1, inplace=True)\n",
    "        all_data.drop(\"EmployeeCount\", axis=1, inplace=True)\n",
    "        all_data.drop(\"StandardHours\", axis=1, inplace=True)\n",
    "\n",
    "    if type == 2:\n",
    "        # drop unethical columns\n",
    "        all_data.drop(\"Male\", axis=1, inplace=True)\n",
    "        all_data.drop(\"Female\", axis=1, inplace=True)\n",
    "        all_data.drop(\"Single\", axis=1, inplace=True)\n",
    "        all_data.drop(\"Married\", axis=1, inplace=True)\n",
    "        all_data.drop(\"Divorced\", axis=1, inplace=True)\n",
    "        all_data.drop(\"Age\", axis=1, inplace=True)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "all_data = dataCleaning(all_data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(all_data.corr(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 10))\n",
    "all_data_corr = all_data.corr()\n",
    "sns.heatmap(all_data_corr[['Attrition']].sort_values(by=['Attrition'],ascending=False), vmin=-1, vmax=1, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lien entre age et temps travaillé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_hours_worked = all_data.groupby(['Age'])[\"avg_hours_worked\"].mean()\n",
    "plt.plot(age_hours_worked.index, age_hours_worked.values, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = dataCleaning(all_data,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gestion des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nullToMedian(all_data):\n",
    "    all_data[\"NumCompaniesWorked\"] = all_data[\"NumCompaniesWorked\"].replace(np.nan,median(all_data[\"NumCompaniesWorked\"]))\n",
    "    all_data[\"TotalWorkingYears\"] = all_data[\"TotalWorkingYears\"].replace(np.nan,median(all_data[\"TotalWorkingYears\"]))\n",
    "\n",
    "    all_data[\"EnvironmentSatisfaction\"] = all_data[\"EnvironmentSatisfaction\"].replace(np.nan,median(all_data[\"EnvironmentSatisfaction\"]))\n",
    "    all_data[\"JobSatisfaction\"] = all_data[\"JobSatisfaction\"].replace(np.nan,median(all_data[\"JobSatisfaction\"]))\n",
    "    all_data[\"WorkLifeBalance\"] = all_data[\"WorkLifeBalance\"].replace(np.nan,median(all_data[\"WorkLifeBalance\"]))\n",
    "    return all_data\n",
    "\n",
    "def nullToKnnImputer(all_data):\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    all_data_imputed = pd.DataFrame(imputer.fit_transform(all_data), columns=all_data.columns)\n",
    "    return all_data_imputed\n",
    "\n",
    "all_data = nullToMedian(all_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection des caractéristiques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le test du khi2 permet d'identifier les caractéristiques les plus pertinentes pour un modèle en évaluant leur indépendance par rapport à la variable cible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les données\n",
    "y = all_data[\"Attrition\"]\n",
    "X = all_data.drop(columns=[\"Attrition\"])\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des caractéristiques avec le test du khi2\n",
    "k_best = SelectKBest(chi2, k=\"all\")  # Utilisez un nombre approprié pour 'k' en fonction de vos données\n",
    "X_train_kbest = k_best.fit_transform(x_train, y_train)\n",
    "X_test_kbest = k_best.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir un modèle (ici, nous utilisons la régression logistique)\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Liste pour stocker les scores de validation croisée\n",
    "cv_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = x_train.shape[1]\n",
    "\n",
    "# Boucle sur différentes valeurs de k\n",
    "for k in range(1, max_features + 1):\n",
    "    # Sélection des k meilleures caractéristiques\n",
    "    k_best = SelectKBest(chi2, k=k)\n",
    "    X_train_kbest = k_best.fit_transform(x_train, y_train)\n",
    "    \n",
    "    # Évaluation du modèle avec validation croisée k-fold (k = 5)\n",
    "    scores = cross_val_score(model, X_train_kbest, y_train, cv=5)\n",
    "    mean_score = np.mean(scores)\n",
    "    cv_scores.append(mean_score)\n",
    "\n",
    "# Trouver la valeur de k avec la meilleure performance\n",
    "best_k = np.argmax(cv_scores) + 1\n",
    "print(f\"La meilleure valeur de k est {best_k}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons donc déterminé que la valeur optimale de k est 29, nous garderons donc les 29 caractéristiques les plus importantes. Déterminons maintenant nos données finales à utiliser dans nos modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des caractéristiques avec le test du khi2\n",
    "k_best = SelectKBest(chi2, k=29)\n",
    "X_train_kbest = k_best.fit_transform(x_train, y_train)\n",
    "X_test_kbest = k_best.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation des données de test et d'entrainement en dataframe\n",
    "x_train = pd.DataFrame(X_train_kbest)\n",
    "x_test = pd.DataFrame(X_test_kbest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mise en place des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model, x_train, y_train):\n",
    "    print(\"Accuracy:\", cross_val_score(model, x_train, y_train, cv=7, scoring='accuracy').mean())\n",
    "    print(\"F1 score:\", cross_val_score(model, x_train, y_train, cv=7, scoring='f1').mean())\n",
    "    print(\"Precision:\", cross_val_score(model, x_train, y_train, cv=7, scoring='precision').mean())\n",
    "    print(\"Recall:\", cross_val_score(model, x_train, y_train, cv=7, scoring='recall').mean())\n",
    "    print(\"ROC AUC score:\", cross_val_score(model, x_train, y_train, cv=7, scoring='roc_auc').mean())\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    RocCurveDisplay.from_estimator(dtc, x_test, y_test, ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix des hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le DecisionTreeClassifier nous allons utiliser GridSearch afin de trouver les meilleurs hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dtc = Pipeline(steps=[\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [16,18,20,21,22,23,24]\n",
    "splitter = ['best','random']\n",
    "\n",
    "\n",
    "parameters_dtc = dict(classifier__criterion=criterion,\n",
    "                  classifier__max_depth=max_depth,\n",
    "                  classifier__splitter=splitter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = lambda x: (x-x.min()) / (x.max()-x.min())\n",
    "all_data = all_data.apply(min_max_scaler)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mise en place des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = all_data[\"Attrition\"]\n",
    "\n",
    "X = all_data.drop(columns=[\"Attrition\"])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model, x_train, y_train):\n",
    "    print(\"Accuracy:\", cross_val_score(model, x_train, y_train, cv=7, scoring='accuracy').mean())\n",
    "    print(\"F1 score:\", cross_val_score(model, x_train, y_train, cv=7, scoring='f1').mean())\n",
    "    print(\"Precision:\", cross_val_score(model, x_train, y_train, cv=7, scoring='precision').mean())\n",
    "    print(\"Recall:\", cross_val_score(model, x_train, y_train, cv=7, scoring='recall').mean())\n",
    "    print(\"ROC AUC score:\", cross_val_score(model, x_train, y_train, cv=7, scoring='roc_auc').mean())\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    RocCurveDisplay.from_estimator(model, x_test, y_test, ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix des hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le DecisionTreeClassifier nous allons utiliser GridSearch afin de trouver les meilleurs hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dtc = Pipeline(steps=[\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [16,18,20,21,22,23,24]\n",
    "splitter = ['best','random']\n",
    "\n",
    "\n",
    "parameters_dtc = dict(classifier__criterion=criterion,\n",
    "                  classifier__max_depth=max_depth,\n",
    "                  classifier__splitter=splitter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_GS = GridSearchCV(pipeline_dtc, parameters_dtc, cv=7)\n",
    "dtc_GS.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(dtc_GS.cv_results_).sort_values(by=['rank_test_score']).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(criterion='gini', max_depth=20, splitter=\"random\")\n",
    "\n",
    "dtc.fit(x_train, y_train)\n",
    "\n",
    "score(dtc, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après plusieurs tests, on ressort qu'une max_depth de 22 et un criterion en entropy est le meilleur model que l'on peu avoir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dtc = dtc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_cm = confusion_matrix(y_test, y_pred_dtc)\n",
    "dtc_cmd = ConfusionMatrixDisplay(confusion_matrix=dtc_cm,\n",
    "                       display_labels=[0,1])\n",
    "\n",
    "dtc_cmd.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_hgbc = Pipeline(steps=[\n",
    "    ('classifier', HistGradientBoostingClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.1,0.2]\n",
    "max_depth = [10,15,20]\n",
    "loss = ['log_loss']\n",
    "max_iter = [10,20,50,100]\n",
    "\n",
    "parameters_hgbc = dict(classifier__learning_rate=learning_rate,\n",
    "                  classifier__max_depth=max_depth,\n",
    "                  classifier__loss=loss,\n",
    "                  classifier__max_iter=max_iter\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_GS = GridSearchCV(pipeline_hgbc, parameters_hgbc)\n",
    "hgbc_GS.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(hgbc_GS.cv_results_).sort_values(by=['rank_test_score']).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc = HistGradientBoostingClassifier(learning_rate=0.2, max_depth=20, max_iter=100, loss=\"log_loss\")\n",
    "\n",
    "hgbc.fit(x_train, y_train)\n",
    "\n",
    "score(hgbc, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hgbc = hgbc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_hgbc = confusion_matrix(y_test, y_pred_hgbc)\n",
    "cmd_hgbc = ConfusionMatrixDisplay(confusion_matrix=cm_hgbc,\n",
    "                       display_labels=[0,1])\n",
    "\n",
    "cmd_hgbc.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rfc = Pipeline(steps=[\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [1,2,3,5,10,50,100,200,300]\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [5,10,14,18,22,24,30,50,100]\n",
    "\n",
    "\n",
    "parameters_rfc = dict(classifier__n_estimators=n_estimators,\n",
    "                  classifier__criterion=criterion,\n",
    "                  classifier__max_depth=max_depth\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_GS = GridSearchCV(pipeline_rfc, parameters_rfc)\n",
    "rfc_GS.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(rfc_GS.cv_results_).sort_values(by=['rank_test_score']).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=300,criterion='entropy',max_depth=24)\n",
    "\n",
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "score(rfc, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc = rfc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rfc = confusion_matrix(y_test, y_pred_rfc)\n",
    "cmd_rfc = ConfusionMatrixDisplay(confusion_matrix=cm_rfc,\n",
    "                       display_labels=[0,1])\n",
    "\n",
    "cmd_rfc.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_pe = Pipeline(steps=[\n",
    "    ('classifier', Perceptron())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = ['l2','l1','elasticnet']\n",
    "alpha = [0.0001,0.0002,0.0003]\n",
    "\n",
    "\n",
    "parameters_pe = dict(classifier__penalty=penalty,\n",
    "                  classifier__alpha=alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_GS = GridSearchCV(pipeline_pe, parameters_pe)\n",
    "pe_GS.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(pe_GS.cv_results_).sort_values(by=['rank_test_score']).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = Perceptron(penalty='l2',alpha = 0.0001)\n",
    "\n",
    "pe.fit(x_train, y_train)\n",
    "\n",
    "score(pe, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pe = pe.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_pe = confusion_matrix(y_test, y_pred_pe)\n",
    "cmd_pe = ConfusionMatrixDisplay(confusion_matrix=cm_pe,\n",
    "                       display_labels=[0,1])\n",
    "\n",
    "cmd_pe.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline(steps=[\n",
    "    ('classifier', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = ['l2']\n",
    "max_iter = [100,500,1000]\n",
    "\n",
    "\n",
    "parameters_lr = dict(classifier__penalty = penalty,\n",
    "                  classifier__max_iter = max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_GS = GridSearchCV(pipeline_lr, parameters_lr)\n",
    "lr_GS.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(lr_GS.cv_results_).sort_values(by=['rank_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2', max_iter = 100)\n",
    "\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "score(lr, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "cmd_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr,\n",
    "                       display_labels=[0,1])\n",
    "\n",
    "cmd_lr.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des résultats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_dtc = list(zip(x_train.columns, dtc.feature_importances_))\n",
    "coefs_dtc = pd.DataFrame(cl_dtc, columns = ['Column', 'Coefs'])\n",
    "coefs_dtc.sort_values(by=['Coefs'], ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc.n_trees_per_iteration_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_rfc = list(zip(x_train.columns, rfc.feature_importances_))\n",
    "coefs_rfc= pd.DataFrame(cl_rfc, columns = ['Column', 'Coefs'])\n",
    "coefs_rfc.sort_values(by=['Coefs'], ascending=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_pe = list(zip(x_train.columns, pe.coef_[0]))\n",
    "coefs_pe = pd.DataFrame(cl_pe, columns = ['Column', 'Coefs'])\n",
    "coefs_pe.sort_values(by=['Coefs'], ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_lr = list(zip(x_train.columns, lr.coef_[0]))\n",
    "coefs_lr = pd.DataFrame(cl_lr, columns = ['Column', 'Coefs'])\n",
    "coefs_lr.sort_values(by=['Coefs'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "<div id=\"cc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que l'étude sur le fait de se sentir bien ou non a un fort impact sur les démissions\n",
    "On remarque aussi que le nombre d'heure travaillée a aussi un certain impacte sur les démissions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
